{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Items for ablation study:\n",
        "\n",
        "Shrink the model (Change ```dims``` of ```ODEFunc```)\n",
        "\n",
        "Train for fewer timesteps (Edit ```num_epochs``` variable)\n",
        "\n",
        "Add varied levels of noise (Add noise after grabbing the batch)\n",
        "\n",
        "Use fewer training trajectories (???)"
      ],
      "metadata": {
        "id": "1RHbXaXRHN8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports + Necessary Functions"
      ],
      "metadata": {
        "id": "UXlMG8SAaF3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdiffeq"
      ],
      "metadata": {
        "id": "HojjUqIjPQZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(sys.path[0]+\"\\\\\\\\..\")  # assuming the first element of sys.path is the path to the scripts folder, this allows imports from within double-pendulum\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchdiffeq import odeint\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from scipy.integrate import solve_ivp\n",
        "from torch.utils.data.dataloader import Dataset\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "lf67g6VjPHtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    #So bog basic it should tap for black mana\n",
        "    #DT That one's for you\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(8, 30)\n",
        "        self.linear2 = nn.Linear(30,30)\n",
        "        self.linear3 = nn.Linear(30,30)\n",
        "        self.linear4 = nn.Linear(30,4)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.relu(self.linear3(x))\n",
        "        x = self.linear4(x)\n",
        "        x = torch.clamp(x, -1e6, 1e6)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Pendulum_Data(Dataset):\n",
        "    def __init__(self, min_length=1, max_length=1, G=9.81, delta_t=0.005, size=2**15):\n",
        "        self.__dict__.update(locals())\n",
        "\n",
        "    @staticmethod\n",
        "    def derivatives(t, state, params, G=9.81):\n",
        "        L1, L2, m1, m2 = params\n",
        "        theta1, z1, theta2, z2 = state\n",
        "        delta = theta2 - theta1\n",
        "\n",
        "        denominator1 = (m1 + m2) * L1 - m2 * L1 * np.cos(delta) ** 2\n",
        "        denominator2 = (L2 / L1) * denominator1\n",
        "\n",
        "        dtheta1_dt = z1\n",
        "        dz1_dt = (\n",
        "                (m2 * L1 * z1 ** 2 * np.sin(delta) * np.cos(delta)\n",
        "                 + m2 * G * np.sin(theta2) * np.cos(delta)\n",
        "                 + m2 * L2 * z2 ** 2 * np.sin(delta)\n",
        "                 - (m1 + m2) * G * np.sin(theta1))\n",
        "                / denominator1\n",
        "        )\n",
        "        dtheta2_dt = z2\n",
        "        dz2_dt = (\n",
        "                (-m2 * L2 * z2 ** 2 * np.sin(delta) * np.cos(delta)\n",
        "                 + (m1 + m2) * G * np.sin(theta1) * np.cos(delta)\n",
        "                 - (m1 + m2) * L1 * z1 ** 2 * np.sin(delta)\n",
        "                 - (m1 + m2) * G * np.sin(theta2))\n",
        "                / denominator2\n",
        "        )\n",
        "\n",
        "        return np.array([dtheta1_dt, dz1_dt, dtheta2_dt, dz2_dt])\n",
        "\n",
        "    def run_simulation(self, theta1_init, theta2_init, l1, l2, m1, m2, v1, v2, t_eval):\n",
        "        state_0 = [theta1_init, v1, theta2_init, v2]\n",
        "        # Solve the system\n",
        "        params = [[l1, l2, m1, m2]]\n",
        "        solution = solve_ivp(\n",
        "            Pendulum_Data.derivatives, (0, t_eval[-1]), state_0, t_eval=t_eval, args=params,\n",
        "        )\n",
        "        # Return data as dictionary\n",
        "        return np.array(\n",
        "            [[l1] * len(t_eval), [l2] * len(t_eval), [m1] * len(t_eval), [m2] * len(t_eval), *solution.y, t_eval]).T\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        t_final = (np.random.randint(self.min_length, self.max_length + 1)+1) * self.delta_t  # the extra +1 makes it so the arange has the right number of steps\n",
        "        theta1_init, theta2_init = np.random.uniform(-np.pi, np.pi, 2)\n",
        "        l1, l2 = np.clip(np.random.normal(1, .5, 2), 0.1, 3)\n",
        "        m1, m2 = np.clip(np.random.normal(1, .5, 2), 0.1, 3)\n",
        "        v1, v2 = np.random.normal(size=2)\n",
        "        theta =  self.run_simulation(theta1_init=theta1_init, theta2_init=theta2_init, l1=l1, l2=l2, m1=m1, m2=m2, v1=v1,\n",
        "                                   v2=v2, t_eval=np.arange(0, t_final, self.delta_t))\n",
        "        return theta[:-1], theta[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size"
      ],
      "metadata": {
        "id": "o2kApl8_PJoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ODEFunc(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super(ODEFunc, self).__init__()\n",
        "        self.num_calls = 0\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(5, dims[0]),  # state: (theta1, w1, theta2, w2, t)\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        if len(dims) > 1:\n",
        "            for i in range(1,len(dims)):\n",
        "                self.net.append(nn.Linear(dims[i-1], dims[i]))\n",
        "                self.net.append(nn.Tanh())\n",
        "        self.net.append(nn.Linear(dims[-1], 5))\n",
        "\n",
        "    def forward(self, t, y):\n",
        "        # y has shape (batch_size, 4)\n",
        "        self.num_calls += 1\n",
        "        return self.net(y)"
      ],
      "metadata": {
        "id": "5uMJynTgQ2v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "-oUE3Rq9aT8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(num_epochs, dims, std_dev):\n",
        "    min_steps = 1\n",
        "    max_steps = 1\n",
        "    G = 9.81\n",
        "    delta_t = 0.005\n",
        "    lr = 1e-3\n",
        "    weight_decay = 1e-4\n",
        "    batch_size = 1\n",
        "    lam = 0.5\n",
        "    save_every = 100\n",
        "    val_every = 25\n",
        "    val_size = 100\n",
        "\n",
        "    # Set model, optimizer and loss\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = ODEFunc()\n",
        "    criterion = nn.MSELoss()\n",
        "    model = model.to(device=device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    losses = []\n",
        "\n",
        "    # Training\n",
        "    data = Pendulum_Data(min_steps, max_steps, G, delta_t, batch_size)\n",
        "    dl = DataLoader(data, batch_size=batch_size)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for i in tqdm(range(num_epochs)):\n",
        "        train_losses_local = []\n",
        "        # each batch is as long as the dataset, so an epoch is one batch\n",
        "        for X_batch, y_batch in dl:\n",
        "            X_batch += np.random.normal(0, std_dev, X_batch.shape)\n",
        "            X_batch, y_batch = X_batch.to(torch.float), y_batch.to(torch.float)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # get prediction and make a tuple to pass to the loss\n",
        "            y_pred = odeint(model, X_batch[:, :, 4:], delta_t * torch.arange(len(X_batch)+1))[1:]\n",
        "            y_batch = y_batch[:, :, 4:]  # remove mass/length information from outputs\n",
        "            loss_args = (y_pred.squeeze(), y_batch.squeeze())\n",
        "\n",
        "            # calculate loss and backprop\n",
        "            loss = criterion(*loss_args)\n",
        "            loss.backward()\n",
        "            train_losses_local.append(loss.item())\n",
        "            optimizer.step()\n",
        "        train_losses.append(np.mean(train_losses_local))\n",
        "\n",
        "        if (i + 1) % val_every == 0:\n",
        "            model.eval()\n",
        "            val_losses_local = []\n",
        "\n",
        "            # run val_size batches to test validation error\n",
        "            for j in range(val_size):\n",
        "                for X_batch, y_batch in dl:\n",
        "                    X_batch, y_batch = X_batch.to(torch.float), y_batch.to(torch.float)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # get prediction\n",
        "                    y_pred = None\n",
        "                    loss_args = None\n",
        "                    if model == 'PINN':\n",
        "                        # the last element of X_batch is time, and should not be included\n",
        "                        y_pred = model(X_batch[:, :, :-1])\n",
        "                        loss_args = (y_pred, y_batch[:, :, 4:-1], X_batch[:, :, 4:-1])\n",
        "                    else:\n",
        "                        y_pred = odeint(model, X_batch[:, :, 4:], delta_t * torch.arange(len(X_batch) + 1))[1:]\n",
        "                        y_batch = y_batch[:, :, 4:]  # remove mass/length information from outputs\n",
        "                        loss_args = (y_pred.squeeze(), y_batch.squeeze())\n",
        "\n",
        "                    # calculate loss, then average\n",
        "                    loss = criterion(*loss_args)\n",
        "                    val_losses_local.append(loss.item())\n",
        "            val_losses.append(np.mean(val_losses_local))\n",
        "            model.train()\n",
        "\n",
        "        # if (i + 1) % save_every == 0:\n",
        "            # torch.save(model.state_dict(), f'{path}_{i}.pt')\n",
        "            # torch.save(train_losses, f'{path}_{i}_train_losses.pt')\n",
        "            # torch.save(val_losses, f'{path}_{i}_val_losses.pt')\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "57Do_abiPAvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "aQoFofD3HHSg",
        "outputId": "e6b3789a-4685-4c3b-a0e7-90d0c252ae60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ODEFunc dimensions: [64, 128, 64]\n",
            "Noise variance: 0\n",
            "Epochs: 200\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3b9214dd11ef>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
          ]
        }
      ],
      "source": [
        "epochs = [200, 500, 1000]\n",
        "dim_sets = [[64,128,64],[64,64],[32]]\n",
        "noise = [0, 0.05, 0.5]\n",
        "\n",
        "for dims in dim_sets:\n",
        "    print(\"ODEFunc dimensions:\", dims)\n",
        "    for std_dev in noise:\n",
        "        print(\"Noise variance:\", std_dev)\n",
        "        for num_epochs in epochs:\n",
        "            print(\"Epochs:\", num_epochs)\n",
        "            train_losses, val_losses = train_model(num_epochs, dims, std_dev)\n",
        "            plt.subplot(121)\n",
        "            plt.plot(train_losses)\n",
        "            plt.title(\"Training Loss\")\n",
        "            plt.subplot(122)\n",
        "            plt.plot(val_losses)\n",
        "            plt.title(\"Validation Loss\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            print('\\n')"
      ]
    }
  ]
}