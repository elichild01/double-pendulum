{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pdb\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With heavy help from https://github.com/benmoseley/harmonic-oscillator-pinn-workshop/blob/main/PINN_intro_workshop.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    #So bog basic it should tap for black mana\n",
    "    #DT That one's for you\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(8, 30)\n",
    "        self.linear2 = nn.Linear(30,30)\n",
    "        self.linear3 = nn.Linear(30,30)\n",
    "        self.linear4 = nn.Linear(30,8)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        x = torch.clamp(x, -1e6, 1e6)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data into a nice form\n",
    "with open('../data/simulations-zero_initial_velocity-varied_mass-varied_length-small.json', 'r') as file:\n",
    "    raw_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gonna want inputs, a list of 8d vectors; and outputs, a list of 8d vectors that come immediately after inputs\n",
    "inputs = []\n",
    "outputs = []\n",
    "np.random.seed(522)\n",
    "ts_indices = np.random.choice(len(raw_data), replace=False, size=len(raw_data)//4)\n",
    "#Use a quarter of all time series generated 'cause there's a lot\n",
    "for i in ts_indices:\n",
    "    timeseries = raw_data[i]\n",
    "    n_tsteps = timeseries['num_tsteps']\n",
    "    #Also, use a quarter of all time steps\n",
    "    tstep_indices = np.random.choice(n_tsteps-1, replace=False, size=n_tsteps//4)\n",
    "    input_vecs = np.empty((len(tstep_indices), 8))\n",
    "    output_vecs = np.empty((len(tstep_indices), 8))\n",
    "    for j, k in enumerate(tstep_indices):\n",
    "        input_vecs[j, 4:] = np.array([timeseries['theta1'][k], timeseries['theta2'][k], timeseries['theta1prime'][k], timeseries['theta2prime'][k]])\n",
    "        output_vecs[j, 4:] = np.array([timeseries['theta1'][k+1], timeseries['theta2'][k+1], timeseries['theta1prime'][k+1], timeseries['theta2prime'][k+1]])\n",
    "    input_vecs[:, :4] = np.array([timeseries['length1'], timeseries['length2'], timeseries['mass1'], timeseries['mass2']])\n",
    "    output_vecs[:, :4] = np.array([timeseries['length1'], timeseries['length2'], timeseries['mass1'], timeseries['mass2']])\n",
    "    for j in range(len(input_vecs)):\n",
    "        inputs.append(list(input_vecs[j]))\n",
    "        outputs.append(list(output_vecs[j]))\n",
    "    \n",
    "test_indices = np.random.choice(len(inputs), replace=False, size = len(inputs)//5)\n",
    "remaining_indices = set(range(len(inputs))) - set(test_indices)\n",
    "val_indices = np.random.choice(list(remaining_indices), replace=False, size=len(inputs)//5)\n",
    "train_indices = list(set(range(len(inputs))) - set(test_indices) - set(val_indices))\n",
    "\n",
    "assert set(train_indices).union(set(test_indices)).union(set(val_indices)) == set(range(len(inputs)))\n",
    "assert len(set(train_indices).intersection(set(test_indices))) == 0\n",
    "assert len(set(train_indices).intersection(set(val_indices))) == 0\n",
    "assert len(set(test_indices).intersection(set(val_indices))) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.tensor(np.array(inputs)[train_indices])\n",
    "train_y = torch.tensor(np.array(outputs)[train_indices])\n",
    "test_X = torch.tensor(np.array(inputs)[test_indices])\n",
    "test_y = torch.tensor(np.array(outputs)[test_indices])\n",
    "val_X = torch.tensor(np.array(inputs)[val_indices])\n",
    "val_y = torch.tensor(np.array(outputs)[val_indices])\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "train_dataset = data.TensorDataset(train_X, train_y)\n",
    "test_dataset = data.TensorDataset(test_X, test_y)\n",
    "val_dataset = data.TensorDataset(val_X, val_y)\n",
    "train_loader = data.DataLoader(train_dataset, shuffle=False, batch_size=batch_size) #Don't need to shuffle 'cause it's already randomly ordered\n",
    "test_loader = data.DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "val_loader = data.DataLoader(val_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 9.81\n",
    "def derivatives(t, state):\n",
    "    if torch.nan in state: pdb.set_trace()\n",
    "    if torch.inf in state or -torch.inf in state: pdb.set_trace()\n",
    "    L1, L2, m1, m2, theta1, z1, theta2, z2 = torch.split(state, 1, dim=1)\n",
    "    delta = theta2 - theta1\n",
    "    if torch.nan in delta: pdb.set_trace()\n",
    "    if torch.inf in delta or -torch.inf in delta: pdb.set_trace()\n",
    "\n",
    "    denominator1 = (m1 + m2) * L1 - m2 * L1 * np.cos(delta) ** 2\n",
    "    denominator2 = (L2 / L1) * denominator1\n",
    "\n",
    "    dtheta1_dt = z1\n",
    "    dz1_dt = (\n",
    "        (m2 * L1 * z1 ** 2 * np.sin(delta) * np.cos(delta)\n",
    "         + m2 * g * np.sin(theta2) * np.cos(delta)\n",
    "         + m2 * L2 * z2 ** 2 * np.sin(delta)\n",
    "         - (m1 + m2) * g * np.sin(theta1))\n",
    "        / denominator1\n",
    "    )\n",
    "    if torch.nan in dz1_dt: pdb.set_trace()\n",
    "    dtheta2_dt = z2\n",
    "    dz2_dt = (\n",
    "        (-m2 * L2 * z2 ** 2 * np.sin(delta) * np.cos(delta)\n",
    "         + (m1 + m2) * g * np.sin(theta1) * np.cos(delta)\n",
    "         - (m1 + m2) * L1 * z1 ** 2 * np.sin(delta)\n",
    "         - (m1 + m2) * g * np.sin(theta2))\n",
    "        / denominator2\n",
    "    )\n",
    "\n",
    "    batched_zeros = np.zeros_like(dtheta1_dt)\n",
    "    return np.column_stack([batched_zeros, batched_zeros, batched_zeros, batched_zeros, dtheta1_dt, dz1_dt, dtheta2_dt, dz2_dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rk4_derivs(state, f, h):\n",
    "    t = 0\n",
    "    k1 = f(t, state)\n",
    "    k2 = f(t + h*0.5, state + k1*h*0.5)\n",
    "    k3 = f(t + h*0.5, state + k2*h*0.5)\n",
    "    k4 = f(t + h, state + k3*h)\n",
    "    return (k1 + 2*k2 + 2*k3 + k4) / 6\n",
    "\n",
    "class PINNLoss(nn.Module):\n",
    "    def __init__(self, tstep, lamb=.5):\n",
    "        super().__init__()\n",
    "        self.tstep = tstep\n",
    "        self.lamb = lamb\n",
    "        self.data_crit = nn.MSELoss()\n",
    "        self.physics_crit = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true, input_data):\n",
    "        data_loss = self.data_crit(y_pred, y_true)\n",
    "        \n",
    "        empirical_derivs = (y_pred - input_data) / self.tstep\n",
    "        true_derivs = torch.tensor(rk4_derivs(input_data, derivatives, self.tstep))\n",
    "        physics_loss = self.physics_crit(empirical_derivs, true_derivs)\n",
    "\n",
    "        return self.lamb * data_loss + (1-self.lamb) * physics_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [10:14<00:00, 307.20s/it]\n"
     ]
    }
   ],
   "source": [
    "timestep = 10/1999\n",
    "pinn = FeedForward()\n",
    "optimizer = torch.optim.Adam(pinn.parameters(), lr=1e-4)\n",
    "lamb = 0.5 #Test different values for this?\n",
    "criterion = PINNLoss(timestep, lamb=lamb)\n",
    "num_epochs = 2\n",
    "val_every = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for i in tqdm(range(num_epochs)):\n",
    "    train_losses_local = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # if torch.inf in X_batch or -torch.inf in y_batch:\n",
    "        #     print('Found inf in data!')\n",
    "        #     pdb.set_trace()\n",
    "        X_batch, y_batch = X_batch.to(torch.float), y_batch.to(torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = pinn(X_batch)\n",
    "        # empirical_derivs = (output_vector - input_data) / timestep\n",
    "        # physics_loss = (derivs - derivatives(input_data))**2 #Divide by N elns?\n",
    "        # data_loss = lamb*(true_output - output_vector)**2\n",
    "        # loss = physics_loss + data_loss\n",
    "        loss = criterion(y_pred, y_batch, X_batch)\n",
    "        loss.backward()\n",
    "        train_losses_local.append(loss.item())\n",
    "        optimizer.step()\n",
    "    train_losses.append(np.mean(train_losses_local))\n",
    "\n",
    "    if (i+1) % val_every == 0:\n",
    "        pinn.eval()\n",
    "        val_losses_local = []\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(torch.float), y_batch.to(torch.float)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = pinn(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_losses_local.append(loss.item())\n",
    "        val_losses.append(np.mean(val_losses_local))\n",
    "        pinn.train()\n",
    "    #Gotta figure out the data-loading at some point...\n",
    "    #pinn() is supposed to approximate, as a function, the next timestep given the state of a system, and training data with consistent timesteps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
